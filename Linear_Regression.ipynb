{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression generally have the form of $Y_{i} = \\theta_{0} + \\theta_{1} x_{1} + \\theta_{2} x_{2} + ...$ <br>\n",
    "There are several ways to find the coefficients of the regression: <br>\n",
    "1. Linear Algebra: $\\hat{\\theta} = (X^{T}X)^{-1}X^{T}Y$ (When X is invertible) <br>\n",
    "2. Gradient Descent: In this case, we need to write out the loss function and try to minimize the loss. <br>\n",
    "$\\hspace{30mm}$ $F(x)$ = Loss Function = SE = $ \\sum^{n}_{i=1} (Y_{i} - \\hat{Y_{i}})^{2}$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Regression():\n",
    "    def __init__(self, alpha = 1e-10 , num_iter = 10000, early_stop = 1e-50, intercept = True, init_weight = None):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "            Some initializations, if neccesary\n",
    "            \n",
    "            attributes: \n",
    "                        alpha: Learning Rate, default 1e-10\n",
    "                        num_iter: Number of Iterations to update coefficient with training data\n",
    "                        early_stop: Constant control early_stop.\n",
    "                        intercept: Bool, If we are going to fit a intercept, default True.\n",
    "                        init_weight: Matrix (n x 1), input init_weight for testing.\n",
    "                        \n",
    "            \n",
    "            TODO: 1. Initialize all variables needed.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model_name = 'Linear Regression'\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.num_iter = num_iter\n",
    "        self.early_stop = early_stop\n",
    "        self.intercept = intercept\n",
    "        self.init_weight = init_weight  ### For testing correctness.\n",
    "        \n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "            Save the datasets in our model, and perform gradient descent.\n",
    "            \n",
    "            Parameter:\n",
    "                X_train: Matrix or 2-D array. Input feature matrix.\n",
    "                Y_train: Matrix or 2-D array. Input target value.\n",
    "                \n",
    "                \n",
    "                TODO: 2. If we are going to fit the intercept, add a col with all 1's to the first column. (hint: np.hstack, np.ones)\n",
    "                      3. Initilaize our coef with uniform from [-1, 1] with the number of col in training set.\n",
    "                      4. Call the gradient_descent function to train.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.intercept:\n",
    "            ones_column = np.ones((X_train.shape[0], 1))\n",
    "            self.X = np.hstack((ones_column, X_train))\n",
    "        else:\n",
    "            self.X = np.mat(X_train)       \n",
    "        \n",
    "        self.y = np.mat(y_train).T\n",
    "        \n",
    "\n",
    "\n",
    "        if not self.init_weight is None:\n",
    "            self.coef = self.init_weight #### Please change this after you get the example right.\n",
    "        else:\n",
    "            self.coef = np.mat(np.random.uniform(-1,1,self.X.shape[1])).T\n",
    "        self.gradient_descent()\n",
    "\n",
    "\n",
    "    def gradient(self):\n",
    "        \"\"\"\n",
    "            Helper function to calculate the gradient respect to coefficient.\n",
    "            \n",
    "            TODO: 5. Think about the matrix format of the gradient of the loss function.\n",
    "           \n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        #XT(Xw - y)\n",
    "        self.grad_coef = -2 * self.X.T @ (self.y - self.X @ self.coef) \n",
    "        return self.grad_coef\n",
    "        \n",
    "        \n",
    "        \n",
    "    def gradient_descent(self):\n",
    "        \n",
    "        \"\"\"\n",
    "            Training function\n",
    "            \n",
    "            TODO: 6. Calculate the loss with current coefficients.\n",
    "                  7. Update the temp_coef with learning rate and gradient.\n",
    "                  8. Calculate the loss with temp_coef.\n",
    "                  9. Implement the self adeptive learning rate. \n",
    "                      a. If current error is less than previous error, increase learning rate by a factor 1.3. \n",
    "                         And update coef, with temp_coef.\n",
    "                      b. If previous error is less than current error, decrease learning rate by a factor of 0.9.\n",
    "                         Don't update coef.\n",
    "                  10. Add the loss to loss list we create.\n",
    "        \"\"\"\n",
    "\n",
    "        self.loss = []\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            self.gradient()\n",
    "\n",
    "            previous_y_hat = self.X @ self.coef\n",
    "                                                                                                    \n",
    "            temp_coef = self.coef - self.alpha * self.grad_coef\n",
    "            \n",
    "            ones = np.mat(np.ones((1,self.X.shape[0])))  # Matrix with 1's (1 x n), help with calculate the sum of a mattrix. hint: Think about dot product.\n",
    "            \n",
    "            pre_error = ones @ np.square(self.y - previous_y_hat)\n",
    "            \n",
    "            current_error = ones @ np.square(self.y - self.X @ temp_coef)\n",
    "            \n",
    "            ### This is the early stop, don't modify fllowing three lines.\n",
    "            if (abs(pre_error - current_error) < self.early_stop) | (abs(abs(pre_error - current_error) / pre_error) < self.early_stop):\n",
    "                self.coef = temp_coef\n",
    "                return self\n",
    "            \n",
    "            if current_error <= pre_error:\n",
    "                self.coef = temp_coef\n",
    "                self.alpha = self.alpha * 1.3\n",
    "            else:\n",
    "                self.alpha = self.alpha * 0.9\n",
    "                \n",
    "            self.loss.append(current_error[0,0])\n",
    "            \n",
    "            if i % 10000 == 0:\n",
    "                print('Iteration: ' +  str(i))\n",
    "                print('Coef: '+ str(self.coef))\n",
    "                print('Loss: ' + str(current_error))            \n",
    "        return self\n",
    "    \n",
    "    def ind_predict(self, x: list):\n",
    "        \"\"\"\n",
    "            Predict the value based on its feature vector x.\n",
    "\n",
    "            Parameter:\n",
    "            x: Matrix, array or list. Input feature point.\n",
    "            \n",
    "            Return:\n",
    "                result: prediction of given data point\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "            TODO: 11. Implement the prediction function\n",
    "        \"\"\"\n",
    "        result = np.matrix(x).reshape((1, -1)) @ self.coef\n",
    "        \n",
    "        return result[0,0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            X is a matrix or 2-D numpy array, represnting testing instances. \n",
    "            Each testing instance is a feature vector. \n",
    "            \n",
    "            Parameter:\n",
    "            X: Matrix, array or list. Input feature point.\n",
    "            \n",
    "            Return:\n",
    "                ret: prediction of given data matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "            TODO: 12. Make sure add the 1's column like we did to add intercept.\n",
    "                  13. Revise the following for-loop to call ind_predict to get predictions.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        ret = []\n",
    "        X = np.mat(X)\n",
    "        if self.intercept:\n",
    "            ones = np.ones((X.shape[0],1))\n",
    "            X = np.hstack([ones, X])\n",
    "        for x in X:\n",
    "            ret.append(self.ind_predict(x))\n",
    "        return ret\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normaliz(lst):\n",
    "    \"\"\"\n",
    "    Helper function for normalize for faster training.\n",
    "    \"\"\"\n",
    "    maximum = np.max(lst)\n",
    "    minimum = np.min(lst)\n",
    "\n",
    "    return (lst - minimum) / (maximum - minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We generate some easy data for testing. We should fit a line with, $Y = 30 * X + 20$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(np.mat(np.arange(1, 1000, 5)).T)\n",
    "y = np.array((30 * X)).flatten() +  20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do NOT modify the following line, just run it when you are done.  You can also try different initialization, you will notice different coef at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Linear_Regression(alpha = 1, num_iter = 10000000, init_weight= np.mat([15,25]).T)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  As the number of iteration increase, you should notice the coeficient converges to [20, 30]. \n",
    "#### It maybe very slow update. Feel free to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[15.36162795],\n",
       "        [30.00698456]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   45.36861251,   195.40353534,   345.43845816,   495.47338098,\n",
       "         645.5083038 ,   795.54322662,   945.57814944,  1095.61307226,\n",
       "        1245.64799509,  1395.68291791,  1545.71784073,  1695.75276355,\n",
       "        1845.78768637,  1995.82260919,  2145.85753201,  2295.89245483,\n",
       "        2445.92737766,  2595.96230048,  2745.9972233 ,  2896.03214612,\n",
       "        3046.06706894,  3196.10199176,  3346.13691458,  3496.1718374 ,\n",
       "        3646.20676023,  3796.24168305,  3946.27660587,  4096.31152869,\n",
       "        4246.34645151,  4396.38137433,  4546.41629715,  4696.45121998,\n",
       "        4846.4861428 ,  4996.52106562,  5146.55598844,  5296.59091126,\n",
       "        5446.62583408,  5596.6607569 ,  5746.69567972,  5896.73060255,\n",
       "        6046.76552537,  6196.80044819,  6346.83537101,  6496.87029383,\n",
       "        6646.90521665,  6796.94013947,  6946.9750623 ,  7097.00998512,\n",
       "        7247.04490794,  7397.07983076,  7547.11475358,  7697.1496764 ,\n",
       "        7847.18459922,  7997.21952204,  8147.25444487,  8297.28936769,\n",
       "        8447.32429051,  8597.35921333,  8747.39413615,  8897.42905897,\n",
       "        9047.46398179,  9197.49890461,  9347.53382744,  9497.56875026,\n",
       "        9647.60367308,  9797.6385959 ,  9947.67351872, 10097.70844154,\n",
       "       10247.74336436, 10397.77828719, 10547.81321001, 10697.84813283,\n",
       "       10847.88305565, 10997.91797847, 11147.95290129, 11297.98782411,\n",
       "       11448.02274693, 11598.05766976, 11748.09259258, 11898.1275154 ,\n",
       "       12048.16243822, 12198.19736104, 12348.23228386, 12498.26720668,\n",
       "       12648.3021295 , 12798.33705233, 12948.37197515, 13098.40689797,\n",
       "       13248.44182079, 13398.47674361, 13548.51166643, 13698.54658925,\n",
       "       13848.58151208, 13998.6164349 , 14148.65135772, 14298.68628054,\n",
       "       14448.72120336, 14598.75612618, 14748.791049  , 14898.82597182,\n",
       "       15048.86089465, 15198.89581747, 15348.93074029, 15498.96566311,\n",
       "       15649.00058593, 15799.03550875, 15949.07043157, 16099.1053544 ,\n",
       "       16249.14027722, 16399.17520004, 16549.21012286, 16699.24504568,\n",
       "       16849.2799685 , 16999.31489132, 17149.34981414, 17299.38473697,\n",
       "       17449.41965979, 17599.45458261, 17749.48950543, 17899.52442825,\n",
       "       18049.55935107, 18199.59427389, 18349.62919671, 18499.66411954,\n",
       "       18649.69904236, 18799.73396518, 18949.768888  , 19099.80381082,\n",
       "       19249.83873364, 19399.87365646, 19549.90857929, 19699.94350211,\n",
       "       19849.97842493, 20000.01334775, 20150.04827057, 20300.08319339,\n",
       "       20450.11811621, 20600.15303903, 20750.18796186, 20900.22288468,\n",
       "       21050.2578075 , 21200.29273032, 21350.32765314, 21500.36257596,\n",
       "       21650.39749878, 21800.43242161, 21950.46734443, 22100.50226725,\n",
       "       22250.53719007, 22400.57211289, 22550.60703571, 22700.64195853,\n",
       "       22850.67688135, 23000.71180418, 23150.746727  , 23300.78164982,\n",
       "       23450.81657264, 23600.85149546, 23750.88641828, 23900.9213411 ,\n",
       "       24050.95626392, 24200.99118675, 24351.02610957, 24501.06103239,\n",
       "       24651.09595521, 24801.13087803, 24951.16580085, 25101.20072367,\n",
       "       25251.2356465 , 25401.27056932, 25551.30549214, 25701.34041496,\n",
       "       25851.37533778, 26001.4102606 , 26151.44518342, 26301.48010624,\n",
       "       26451.51502907, 26601.54995189, 26751.58487471, 26901.61979753,\n",
       "       27051.65472035, 27201.68964317, 27351.72456599, 27501.75948882,\n",
       "       27651.79441164, 27801.82933446, 27951.86425728, 28101.8991801 ,\n",
       "       28251.93410292, 28401.96902574, 28552.00394856, 28702.03887139,\n",
       "       28852.07379421, 29002.10871703, 29152.14363985, 29302.17856267,\n",
       "       29452.21348549, 29602.24840831, 29752.28333113, 29902.31825396])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(clf.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.00502513],\n",
       "       [0.01005025],\n",
       "       [0.01507538],\n",
       "       [0.0201005 ],\n",
       "       [0.02512563],\n",
       "       [0.03015075],\n",
       "       [0.03517588],\n",
       "       [0.04020101],\n",
       "       [0.04522613],\n",
       "       [0.05025126],\n",
       "       [0.05527638],\n",
       "       [0.06030151],\n",
       "       [0.06532663],\n",
       "       [0.07035176],\n",
       "       [0.07537688],\n",
       "       [0.08040201],\n",
       "       [0.08542714],\n",
       "       [0.09045226],\n",
       "       [0.09547739],\n",
       "       [0.10050251],\n",
       "       [0.10552764],\n",
       "       [0.11055276],\n",
       "       [0.11557789],\n",
       "       [0.12060302],\n",
       "       [0.12562814],\n",
       "       [0.13065327],\n",
       "       [0.13567839],\n",
       "       [0.14070352],\n",
       "       [0.14572864],\n",
       "       [0.15075377],\n",
       "       [0.15577889],\n",
       "       [0.16080402],\n",
       "       [0.16582915],\n",
       "       [0.17085427],\n",
       "       [0.1758794 ],\n",
       "       [0.18090452],\n",
       "       [0.18592965],\n",
       "       [0.19095477],\n",
       "       [0.1959799 ],\n",
       "       [0.20100503],\n",
       "       [0.20603015],\n",
       "       [0.21105528],\n",
       "       [0.2160804 ],\n",
       "       [0.22110553],\n",
       "       [0.22613065],\n",
       "       [0.23115578],\n",
       "       [0.2361809 ],\n",
       "       [0.24120603],\n",
       "       [0.24623116],\n",
       "       [0.25125628],\n",
       "       [0.25628141],\n",
       "       [0.26130653],\n",
       "       [0.26633166],\n",
       "       [0.27135678],\n",
       "       [0.27638191],\n",
       "       [0.28140704],\n",
       "       [0.28643216],\n",
       "       [0.29145729],\n",
       "       [0.29648241],\n",
       "       [0.30150754],\n",
       "       [0.30653266],\n",
       "       [0.31155779],\n",
       "       [0.31658291],\n",
       "       [0.32160804],\n",
       "       [0.32663317],\n",
       "       [0.33165829],\n",
       "       [0.33668342],\n",
       "       [0.34170854],\n",
       "       [0.34673367],\n",
       "       [0.35175879],\n",
       "       [0.35678392],\n",
       "       [0.36180905],\n",
       "       [0.36683417],\n",
       "       [0.3718593 ],\n",
       "       [0.37688442],\n",
       "       [0.38190955],\n",
       "       [0.38693467],\n",
       "       [0.3919598 ],\n",
       "       [0.39698492],\n",
       "       [0.40201005],\n",
       "       [0.40703518],\n",
       "       [0.4120603 ],\n",
       "       [0.41708543],\n",
       "       [0.42211055],\n",
       "       [0.42713568],\n",
       "       [0.4321608 ],\n",
       "       [0.43718593],\n",
       "       [0.44221106],\n",
       "       [0.44723618],\n",
       "       [0.45226131],\n",
       "       [0.45728643],\n",
       "       [0.46231156],\n",
       "       [0.46733668],\n",
       "       [0.47236181],\n",
       "       [0.47738693],\n",
       "       [0.48241206],\n",
       "       [0.48743719],\n",
       "       [0.49246231],\n",
       "       [0.49748744],\n",
       "       [0.50251256],\n",
       "       [0.50753769],\n",
       "       [0.51256281],\n",
       "       [0.51758794],\n",
       "       [0.52261307],\n",
       "       [0.52763819],\n",
       "       [0.53266332],\n",
       "       [0.53768844],\n",
       "       [0.54271357],\n",
       "       [0.54773869],\n",
       "       [0.55276382],\n",
       "       [0.55778894],\n",
       "       [0.56281407],\n",
       "       [0.5678392 ],\n",
       "       [0.57286432],\n",
       "       [0.57788945],\n",
       "       [0.58291457],\n",
       "       [0.5879397 ],\n",
       "       [0.59296482],\n",
       "       [0.59798995],\n",
       "       [0.60301508],\n",
       "       [0.6080402 ],\n",
       "       [0.61306533],\n",
       "       [0.61809045],\n",
       "       [0.62311558],\n",
       "       [0.6281407 ],\n",
       "       [0.63316583],\n",
       "       [0.63819095],\n",
       "       [0.64321608],\n",
       "       [0.64824121],\n",
       "       [0.65326633],\n",
       "       [0.65829146],\n",
       "       [0.66331658],\n",
       "       [0.66834171],\n",
       "       [0.67336683],\n",
       "       [0.67839196],\n",
       "       [0.68341709],\n",
       "       [0.68844221],\n",
       "       [0.69346734],\n",
       "       [0.69849246],\n",
       "       [0.70351759],\n",
       "       [0.70854271],\n",
       "       [0.71356784],\n",
       "       [0.71859296],\n",
       "       [0.72361809],\n",
       "       [0.72864322],\n",
       "       [0.73366834],\n",
       "       [0.73869347],\n",
       "       [0.74371859],\n",
       "       [0.74874372],\n",
       "       [0.75376884],\n",
       "       [0.75879397],\n",
       "       [0.7638191 ],\n",
       "       [0.76884422],\n",
       "       [0.77386935],\n",
       "       [0.77889447],\n",
       "       [0.7839196 ],\n",
       "       [0.78894472],\n",
       "       [0.79396985],\n",
       "       [0.79899497],\n",
       "       [0.8040201 ],\n",
       "       [0.80904523],\n",
       "       [0.81407035],\n",
       "       [0.81909548],\n",
       "       [0.8241206 ],\n",
       "       [0.82914573],\n",
       "       [0.83417085],\n",
       "       [0.83919598],\n",
       "       [0.84422111],\n",
       "       [0.84924623],\n",
       "       [0.85427136],\n",
       "       [0.85929648],\n",
       "       [0.86432161],\n",
       "       [0.86934673],\n",
       "       [0.87437186],\n",
       "       [0.87939698],\n",
       "       [0.88442211],\n",
       "       [0.88944724],\n",
       "       [0.89447236],\n",
       "       [0.89949749],\n",
       "       [0.90452261],\n",
       "       [0.90954774],\n",
       "       [0.91457286],\n",
       "       [0.91959799],\n",
       "       [0.92462312],\n",
       "       [0.92964824],\n",
       "       [0.93467337],\n",
       "       [0.93969849],\n",
       "       [0.94472362],\n",
       "       [0.94974874],\n",
       "       [0.95477387],\n",
       "       [0.95979899],\n",
       "       [0.96482412],\n",
       "       [0.96984925],\n",
       "       [0.97487437],\n",
       "       [0.9798995 ],\n",
       "       [0.98492462],\n",
       "       [0.98994975],\n",
       "       [0.99497487],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_normaliz(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please try to normalize the X and fit again with normalized X. You should find something interesting. Also think about what you should do for predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Linear_Regression(alpha = 1, num_iter = 10000000, init_weight= np.mat([15,25]).T)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### You can also try this with the wine dataset we use in HW1. Try fit this function to that dataset with same features. If you look closely to the updates of coefficients. What do you find? This could be mentioned in your report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      density  alcohol\n",
      "0     0.99780      9.4\n",
      "1     0.99680      9.8\n",
      "2     0.99700      9.8\n",
      "3     0.99800      9.8\n",
      "4     0.99780      9.4\n",
      "...       ...      ...\n",
      "1594  0.99490     10.5\n",
      "1595  0.99512     11.2\n",
      "1596  0.99574     11.0\n",
      "1597  0.99547     10.2\n",
      "1598  0.99549     11.0\n",
      "\n",
      "[1599 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "url_Wine = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "wine = pd.read_csv(url_Wine, delimiter=';')\n",
    "X = wine[['density','alcohol']]\n",
    "y = wine.quality\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800.6676988774335"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X,y)\n",
    "## Squared Error with sklearn.\n",
    "sum((lr.predict(X) - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You will notice different coefficients, but the loss is very close to each other like 805. In your report, briefly discuss this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Coef: [[-0.01182732]\n",
      " [-0.66646641]\n",
      " [-0.99750631]]\n",
      "Loss: [[5.74302532e+16]]\n",
      "Iteration: 10000\n",
      "Coef: [[0.86062003]\n",
      " [0.21462103]\n",
      " [0.43733026]]\n",
      "Loss: [[816.43547388]]\n",
      "Iteration: 20000\n",
      "Coef: [[1.1174717 ]\n",
      " [0.47777818]\n",
      " [0.3873824 ]]\n",
      "Loss: [[807.0053531]]\n",
      "Iteration: 30000\n",
      "Coef: [[1.20596419]\n",
      " [0.57153019]\n",
      " [0.37033354]]\n",
      "Loss: [[805.8481003]]\n",
      "Iteration: 40000\n",
      "Coef: [[1.2354335 ]\n",
      " [0.60589467]\n",
      " [0.36425607]]\n",
      "Loss: [[805.70605064]]\n",
      "Iteration: 50000\n",
      "Coef: [[1.24421838]\n",
      " [0.6194439 ]\n",
      " [0.3621063 ]]\n",
      "Loss: [[805.68797566]]\n",
      "Iteration: 60000\n",
      "Coef: [[1.24576869]\n",
      " [0.62571401]\n",
      " [0.36137636]]\n",
      "Loss: [[805.68516644]]\n",
      "Iteration: 70000\n",
      "Coef: [[1.24478091]\n",
      " [0.62943014]\n",
      " [0.36111156]]\n",
      "Loss: [[805.68421567]]\n",
      "Iteration: 80000\n",
      "Coef: [[1.24290578]\n",
      " [0.63225327]\n",
      " [0.36102785]]\n",
      "Loss: [[805.68349578]]\n",
      "Iteration: 90000\n",
      "Coef: [[1.24071818]\n",
      " [0.63476353]\n",
      " [0.36099837]]\n",
      "Loss: [[805.68280498]]\n"
     ]
    }
   ],
   "source": [
    "clf = Linear_Regression(alpha = 1, num_iter = 100000, early_stop = 0)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "805.6821176664736"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((clf.predict(X) - y)**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
