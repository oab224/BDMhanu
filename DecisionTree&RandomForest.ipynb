{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6G8uG6sm-gGj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VgqMGh9-gGm"
      },
      "source": [
        "https://en.wikipedia.org/wiki/Decision_tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgiZBDKB-gGn"
      },
      "source": [
        "# Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "N9G43Tqa-gGn"
      },
      "outputs": [],
      "source": [
        "class DecisionTree():\n",
        "    \"\"\"\n",
        "\n",
        "    Decision Tree Classifier\n",
        "\n",
        "    Attributes:\n",
        "        root: Root Node of the tree.\n",
        "        max_depth: Max depth allowed for the tree\n",
        "        size_allowed : Min_size split, smallest size allowed for split\n",
        "        n_features: Number of features to use during building the tree.(Random Forest)\n",
        "        n_split:  Number of split for each feature. (Random Forest)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_depth = 1000, size_allowed = 1, n_features = None, n_split = None):\n",
        "        \"\"\"\n",
        "\n",
        "            Initializations for class attributes.\n",
        "\n",
        "            TODO: 1. Modify the initialization of the attributes of the Decision Tree classifier\n",
        "        \"\"\"\n",
        "\n",
        "        self.root = None\n",
        "        self.max_depth = max_depth\n",
        "        self.size_allowed = size_allowed\n",
        "        self.n_features = n_features\n",
        "        self.n_split = n_split\n",
        "\n",
        "\n",
        "    class Node():\n",
        "        \"\"\"\n",
        "            Node Class for the building the tree.\n",
        "\n",
        "            Attribute:\n",
        "                threshold: The threshold like if x1 < threshold, for spliting.\n",
        "                feature: The index of feature on this current node.\n",
        "                left: Pointer to the node on the left.\n",
        "                right: Pointer to the node on the right.\n",
        "                pure: Bool, describe if this node is pure.\n",
        "                predict: Class, indicate what the most common Y on this node.\n",
        "\n",
        "        \"\"\"\n",
        "        def __init__(self, threshold = None, feature = None):\n",
        "            \"\"\"\n",
        "\n",
        "                Initializations for class attributes.\n",
        "\n",
        "                TODO: 2. Modify the initialization of the attributes of the Node. (Initialize threshold and feature)\n",
        "            \"\"\"\n",
        "\n",
        "\n",
        "            self.threshold = threshold\n",
        "            self.feature = feature\n",
        "            self.left = None\n",
        "            self.right = None\n",
        "            self.pure = False\n",
        "            self.depth = None\n",
        "            self.predict = None\n",
        "\n",
        "\n",
        "    def entropy(self, lst):\n",
        "        \"\"\"\n",
        "            Function Calculate the entropy given lst.\n",
        "\n",
        "            Attributes:\n",
        "                entro: variable store entropy for each step.\n",
        "                classes: all possible classes. (without repeating terms)\n",
        "                counts: counts of each possible classes.\n",
        "                total_counts: number of instances in this lst.\n",
        "\n",
        "            lst is vector of labels.\n",
        "\n",
        "\n",
        "\n",
        "            TODO: 3. Intilize attributes.\n",
        "                  4. Modify and add some codes to the following for-loop\n",
        "                     to compute the correct entropy.\n",
        "                     (make sure count of corresponding label is not 0, think about why we need to do that.)\n",
        "        \"\"\"\n",
        "\n",
        "        entro = 1\n",
        "        classes, counts = np.unique(lst, return_counts = True)\n",
        "        total_counts = counts.sum()\n",
        "        for i in counts:\n",
        "            if True:\n",
        "                i = i / total_counts\n",
        "                entro = entro - (i*np.log2(i))\n",
        "        return entro\n",
        "\n",
        "    def information_gain(self, lst, values, threshold):\n",
        "        \"\"\"\n",
        "\n",
        "            Function Calculate the information gain, by using entropy function.\n",
        "\n",
        "            lst is vector of labels.\n",
        "            values is vector of values for individule feature.\n",
        "            threshold is the split threshold we want to use for calculating the entropy.\n",
        "\n",
        "\n",
        "            TODO:\n",
        "                5. Modify the following variable to calculate the P(left node), P(right node),\n",
        "                   Conditional Entropy(left node) and Conditional Entropy(right node)\n",
        "                6. Return information gain.\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        index = (values < threshold)\n",
        "        left_prop = index.mean()\n",
        "        right_prop = 1 - left_prop\n",
        "        Y_entropy = self.entropy(lst)\n",
        "        left_entropy = self.entropy(lst[index])\n",
        "        right_entropy = self.entropy(lst[np.invert(index)])\n",
        "        #infogain = E - sum prop*entro\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return Y_entropy - left_prop*left_entropy - right_prop*right_entropy\n",
        "\n",
        "    def find_rules(self, data):\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "            Helper function to find the split rules.\n",
        "\n",
        "            data is a matrix or 2-D numpy array, represnting training instances.\n",
        "            Each training instance is a feature vector.\n",
        "\n",
        "            TODO: 7. Modify the following for loop, which loop through each column(feature).\n",
        "                     Find the unique value of each feature, and find the mid point of each adjacent value.\n",
        "                  8. Store them in a list, return that list.\n",
        "\n",
        "        \"\"\"\n",
        "        n, m = data.shape #features\n",
        "        rules = []\n",
        "        for i in data.T:\n",
        "            unique_value = np.unique(i)\n",
        "            diff  = unique_value[:-1] + np.diff(unique_value)/2\n",
        "            rules.append(diff)\n",
        "        return rules\n",
        "\n",
        "    def next_split(self, data, label):\n",
        "        \"\"\"\n",
        "            Helper function to find the split with most information gain, by using find_rules, and information gain.\n",
        "\n",
        "            data is a matrix or 2-D numpy array, represnting training instances.\n",
        "            Each training instance is a feature vector.\n",
        "\n",
        "            label contains the corresponding labels. There might be multiple (i.e., > 2) classes.\n",
        "\n",
        "            TODO: 9. Use find_rules to initialize rules variable\n",
        "                  10. Initialize max_info to some negative number.\n",
        "        \"\"\"\n",
        "\n",
        "        rules = self.find_rules(data)\n",
        "        max_info = -1\n",
        "        num_col = None\n",
        "        threshold = None\n",
        "        entropy_y = self.entropy(label)\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "            TODO: 11. Check Number of features to use, None means all featurs. (Decision Tree always use all feature)\n",
        "                      If n_features is a int, use n_features of features by random choice.\n",
        "                      If n_features == 'sqrt', use sqrt(Total Number of Features ) by random choice.\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        if self.n_features is None :\n",
        "            index_col = np.arange(data.shape[1])\n",
        "        else:\n",
        "            if self.n_features == 'sqrt':\n",
        "                num_index = int(np.sqrt(data.shape[1]))\n",
        "            elif isinstance(self.n_features, int):\n",
        "                num_index = self.n_features\n",
        "            np.random.seed()\n",
        "            index_col = np.random.choice(data.shape[1], num_index, replace = False)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "            TODO: 12. Do the similar selection we did for features, n_split take in None or int or 'sqrt'.\n",
        "                  13. For all selected feature and corresponding rules, we check it's information gain.\n",
        "\n",
        "        \"\"\"\n",
        "        for i in index_col:\n",
        "            count_temp_rules = len(rules[i])\n",
        "\n",
        "            if self.n_split is None :\n",
        "                index_rules = np.arange(count_temp_rules)\n",
        "            else:\n",
        "\n",
        "                if self.n_split == 'sqrt':\n",
        "                    num_rules = int(np.sqrt(count_temp_rules))\n",
        "                elif isinstance(self.n_split, int):\n",
        "                    num_rules = self.n_split\n",
        "                np.random.seed()\n",
        "                index_rules = np.random.choice(count_temp_rules, num_rules, replace = False)\n",
        "\n",
        "            for j in index_rules:\n",
        "                info = self.information_gain(lst = label, values = data[:, i], threshold = rules[i][j])\n",
        "                if info > max_info:\n",
        "                    max_info = info\n",
        "                    num_col = i\n",
        "                    threshold = rules[i][j]\n",
        "        return threshold, num_col\n",
        "\n",
        "    def build_tree(self, X, y, depth):\n",
        "\n",
        "            \"\"\"\n",
        "                Helper function for building the tree.\n",
        "\n",
        "                TODO: 14. First build the root node.\n",
        "            \"\"\"\n",
        "\n",
        "\n",
        "            first_threshold, first_feature = self.next_split(data = X, label = y)\n",
        "            current = self.Node(first_threshold, first_feature)\n",
        "            current.depth = depth\n",
        "\n",
        "            \"\"\"\n",
        "                TODO: 15. Base Case 1: Check if we pass the max_depth, check if the first_feature is None, min split size.\n",
        "                          If some of those condition met, change current to pure, and set predict to the most popular label\n",
        "                          and return current\n",
        "\n",
        "\n",
        "            \"\"\"\n",
        "            if (first_feature is None) or (self.max_depth != None and depth >= self.max_depth) or (self.size_allowed != None and X.shape[0] < self.size_allowed):\n",
        "                labels, cnts = np.unique(y,return_counts = True)\n",
        "                current.predict = np.random.choice(labels[cnts == cnts.max()])\n",
        "                current.pure = True\n",
        "                return current\n",
        "\n",
        "            \"\"\"\n",
        "               Base Case 2: Check if there is only 1 label in this node, change current to pure, and set predict to the label\n",
        "            \"\"\"\n",
        "\n",
        "            if len(np.unique(y)) == 1:\n",
        "                labels, cnts = np.unique(y,return_counts = True)\n",
        "                current.predict = np.random.choice(labels[cnts == cnts.max()])\n",
        "                current.pure = True\n",
        "                return current\n",
        "\n",
        "            \"\"\"\n",
        "                TODO: 16. Find the left node index with feature i <= threshold  Right with feature i > threshold.\n",
        "            \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "            left_index = X[:,first_feature] < first_threshold\n",
        "            right_index = np.invert(left_index)\n",
        "\n",
        "            \"\"\"\n",
        "                TODO: 17. Base Case 3: If we either side is empty, change current to pure, and set predict to the label\n",
        "            \"\"\"\n",
        "            if left_index.sum() == 0 or right_index.sum() == 0 :\n",
        "                labels, cnts = np.unique(y,return_counts = True)\n",
        "                current.predict = np.random.choice(labels[cnts == cnts.max()])\n",
        "                current.pure = True\n",
        "                return current\n",
        "\n",
        "\n",
        "            left_X, left_y = X[left_index,:], y[left_index]\n",
        "            current.left = self.build_tree(left_X, left_y, depth + 1)\n",
        "\n",
        "            right_X, right_y = X[right_index,:], y[right_index]\n",
        "            current.right = self.build_tree(right_X, right_y, depth + 1)\n",
        "\n",
        "            return current\n",
        "\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "            The fit function fits the Decision Tree model based on the training data.\n",
        "\n",
        "            X_train is a matrix or 2-D numpy array, represnting training instances.\n",
        "            Each training instance is a feature vector.\n",
        "\n",
        "            y_train contains the corresponding labels. There might be multiple (i.e., > 2) classes.\n",
        "        \"\"\"\n",
        "        self.root = self.build_tree(X, y, 1)\n",
        "\n",
        "\n",
        "        self.for_running = y[0]\n",
        "        return self\n",
        "\n",
        "    def ind_predict(self, inp):\n",
        "        \"\"\"\n",
        "            Predict the most likely class label of one test instance based on its feature vector x.\n",
        "\n",
        "            TODO: 18. Modify the following while loop to get the prediction.\n",
        "                      Stop condition we are at a node is pure.\n",
        "                      Trace with the threshold and feature.\n",
        "                19. Change return self.for_running to appropiate value.\n",
        "        \"\"\"\n",
        "        cur = self.root\n",
        "        while not cur.pure:\n",
        "\n",
        "            feature = cur.feature\n",
        "            threshold = cur.threshold\n",
        "\n",
        "            if inp[feature] < threshold:\n",
        "                cur = cur.left\n",
        "            else:\n",
        "                cur = cur.right\n",
        "        return cur.predict\n",
        "\n",
        "    def predict(self, inp):\n",
        "        \"\"\"\n",
        "            X is a matrix or 2-D numpy array, represnting testing instances.\n",
        "            Each testing instance is a feature vector.\n",
        "\n",
        "            Return the predictions of all instances in a list.\n",
        "\n",
        "            TODO: 20. Revise the following for-loop to call ind_predict to get predictions.\n",
        "        \"\"\"\n",
        "\n",
        "        result = []\n",
        "        for i in range(inp.shape[0]):\n",
        "            result.append(self.ind_predict(inp[i]))\n",
        "        return result\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boiJ3622-gGp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "V2uPtiFV-gGr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "url_Wine = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
        "wine = pd.read_csv(url_Wine, delimiter=';')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FC1zQF20-gGt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "umUfc-Kj-gGv"
      },
      "outputs": [],
      "source": [
        "X = np.array(wine)[:, :-1]\n",
        "y = np.array(wine)[:, -1]\n",
        "y = np.array(y.flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "g8lOQ1vd-gGx"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5p2aQgpE-gGz"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eACyWCM9-gG1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HMzJB5Ot-gG2",
        "outputId": "50e4605b-7124-4e7b-a106-f4a479b7b6f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.DecisionTree at 0x7ca6b9295120>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "clf = DecisionTree()\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul2dkqPh-gG5"
      },
      "source": [
        "### Train Error should be 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mrinqIMZ-gG6",
        "outputId": "d9f1564c-5834-4a71-8041-01a885e441a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "pred = clf.predict(X_train)\n",
        "(pred == y_train).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMu0yRH1-gG8"
      },
      "source": [
        "### Test Error should be around 0.62"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Mw0_bBUU-gG8"
      },
      "outputs": [],
      "source": [
        "pred = clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "hIVcRBSC-gG-",
        "outputId": "ab6577f4-6ace-4fa4-e058-8550679d8a09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.621875"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "(pred == y_test).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "EuWhhXnn-gG_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSgiIwTc-gHB"
      },
      "source": [
        "https://en.wikipedia.org/wiki/Random_forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "f0gYN1_H-gHB"
      },
      "outputs": [],
      "source": [
        "class RandomForest():\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    RandomForest Classifier\n",
        "\n",
        "    Attributes:\n",
        "        n_trees: Number of trees.\n",
        "        trees: List store each individule tree\n",
        "        n_features: Number of features to use during building each individule tree.\n",
        "        n_split: Number of split for each feature.\n",
        "        max_depth: Max depth allowed for the tree\n",
        "        size_allowed : Min_size split, smallest size allowed for split\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,n_trees = 10, n_features = 'sqrt', n_split = 'sqrt', max_depth = None, size_allowed = 1):\n",
        "\n",
        "        \"\"\"\n",
        "            Initilize all Attributes.\n",
        "\n",
        "            TODO: 1. Intialize n_trees, n_features, n_split, max_depth, size_allowed.\n",
        "        \"\"\"\n",
        "        self.n_trees = n_trees\n",
        "        self.trees = []\n",
        "        self.n_features = n_features\n",
        "        self.n_split = n_split\n",
        "        self.max_depth = max_depth\n",
        "        self.size_allowed = size_allowed\n",
        "\n",
        "\n",
        "    def fit(self, X,y):\n",
        "\n",
        "        \"\"\"\n",
        "            The fit function fits the Random Forest model based on the training data.\n",
        "\n",
        "            X_train is a matrix or 2-D numpy array, represnting training instances.\n",
        "            Each training instance is a feature vector.\n",
        "\n",
        "            y_train contains the corresponding labels. There might be multiple (i.e., > 2) classes.\n",
        "\n",
        "\n",
        "            TODO: 2. Modify the following for loop to create n_trees tress. by calling DecisionTree we created.\n",
        "                     Pass in all the attributes.\n",
        "        \"\"\"\n",
        "        self.for_running = y[4]\n",
        "\n",
        "        for i in range(self.n_trees):\n",
        "            np.random.seed()\n",
        "            temp_clf = DecisionTree(max_depth=self.max_depth, size_allowed = self.size_allowed,n_features = self.n_features, n_split = self.n_split)\n",
        "            temp_clf.fit(X, y)\n",
        "            self.trees.append(temp_clf)\n",
        "        return self\n",
        "\n",
        "    def ind_predict(self, inp):\n",
        "\n",
        "        \"\"\"\n",
        "            Predict the most likely class label of one test instance based on its feature vector x.\n",
        "\n",
        "            TODO: 4. Modify the following code to predict using each Decision Tree.\n",
        "        \"\"\"\n",
        "        result = []\n",
        "        for i in self.trees:\n",
        "            result.append(i.ind_predict(inp))\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "            TODO: 5. Modify the following code to find the correct prediction use majority rule.\n",
        "                     If there is a tie, use random choice to select one of them.\n",
        "        \"\"\"\n",
        "        labels, counts = np.unique(result, return_counts = True)\n",
        "        return np.random.choice(labels[counts == max(counts)])\n",
        "\n",
        "    def predict_all(self, inp):\n",
        "\n",
        "        \"\"\"\n",
        "            X is a matrix or 2-D numpy array, represnting testing instances.\n",
        "            Each testing instance is a feature vector.\n",
        "\n",
        "            Return the predictions of all instances in a list.\n",
        "\n",
        "            TODO: 6. Revise the following for-loop to call ind_predict to get predictions\n",
        "        \"\"\"\n",
        "        result = []\n",
        "        for i in inp:\n",
        "            result.append(self.ind_predict(i))\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-4vXXHnP-gHD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "bg4eRJ58-gHF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZqOzdjh-gHG"
      },
      "source": [
        "### Test Accruacy should be greater than 0.69"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "nRTbwtIG-gHH",
        "outputId": "a08b5fc1-4fd2-4580-ad95-1f37292d7b1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.RandomForest at 0x7ca6bd356f50>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "clf = RandomForest(n_trees= 100, n_split=None)\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Uoq3tFQ_-gHJ",
        "outputId": "b334c509-547c-48ff-cbd0-b12c4eb7b67b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7125"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "pred = clf.predict_all(X_test)\n",
        "(pred == y_test).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUwAMpuT-gHK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}